{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a37f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a9d6b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CharDataset() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m         y = \u001b[38;5;28mself\u001b[39m.data[idx+\u001b[32m1\u001b[39m:idx+\u001b[38;5;28mself\u001b[39m.block_size+\u001b[32m1\u001b[39m]\n\u001b[32m     42\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m train_dataset = \u001b[43mCharDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m val_dataset = CharDataset(val_data, BLOCK_SIZE)\n\u001b[32m     46\u001b[39m train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: CharDataset() takes no arguments"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 64  # Context length\n",
    "MAX_ITERS = 5000\n",
    "LEARNING_RATE = 3e-4\n",
    "EVAL_INTERVAL = 500\n",
    "EMBED_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 4\n",
    "DROPOUT = 0.1\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sample dataset (replace with your text file)\n",
    "text = open('the-verdict.txt', 'r').read()  # Use any text file\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Tokenizer\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [char_to_idx[c] for c in s]\n",
    "decode = lambda l: ''.join([idx_to_char[i] for i in l])\n",
    "\n",
    "# Train/Validation Split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# DataLoader\n",
    "class CharDataset(Dataset):\n",
    "    def _init_(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def _len_(self):\n",
    "        return len(self.data) - self.block_size\n",
    "        \n",
    "    def _getitem_(self, idx):\n",
    "        x = self.data[idx:idx+self.block_size]\n",
    "        y = self.data[idx+1:idx+self.block_size+1]\n",
    "        return x, y\n",
    "\n",
    "train_dataset = CharDataset(train_data, BLOCK_SIZE)\n",
    "val_dataset = CharDataset(val_data, BLOCK_SIZE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def _init_(self, d_model, max_len=5000):\n",
    "        super()._init_()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Multi-Head Self-Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def _init_(self, embed_dim, num_heads):\n",
    "        super()._init_()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        y = (attn @ v).transpose(1, 2).contiguous().reshape(B, T, C)\n",
    "        return self.proj(y)\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def _init_(self, embed_dim, num_heads):\n",
    "        super()._init_()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.ln1(x), mask)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Language Model\n",
    "class MiniLLM(nn.Module):\n",
    "    def _init_(self):\n",
    "        super()._init_()\n",
    "        self.token_embed = nn.Embedding(vocab_size, EMBED_DIM)\n",
    "        self.pos_embed = PositionalEncoding(EMBED_DIM, BLOCK_SIZE)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(EMBED_DIM, NUM_HEADS) for _ in range(NUM_LAYERS)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(EMBED_DIM)\n",
    "        self.head = nn.Linear(EMBED_DIM, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embed(idx)  # (B, T, EMBED_DIM)\n",
    "        x = self.pos_embed(tok_emb)\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = torch.tril(torch.ones(T, T)).view(1, 1, T, T).to(DEVICE)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = nn.functional.cross_entropy(\n",
    "                logits.view(-1, vocab_size), \n",
    "                targets.contiguous().view(-1)\n",
    "            )\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# Initialize model\n",
    "model = MiniLLM().to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "for iter in range(MAX_ITERS):\n",
    "    if iter % EVAL_INTERVAL == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            for i in range(10):  # Approximate validation loss\n",
    "                x, y = val_dataset[np.random.randint(len(val_dataset))]\n",
    "                x, y = x.unsqueeze(0).to(DEVICE), y.unsqueeze(0).to(DEVICE)\n",
    "                _, loss = model(x, y)\n",
    "                val_loss.append(loss.item())\n",
    "            print(f\"Iter {iter}: Val Loss {np.mean(val_loss):.4f}\")\n",
    "    \n",
    "    model.train()\n",
    "    xb, yb = next(iter(train_loader))\n",
    "    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "    _, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Generate text\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
